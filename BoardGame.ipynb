{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d62dba6-b2d4-466c-8812-2d6e5c5a6d24",
   "metadata": {},
   "source": [
    "# Sistem Multi Agen : Board Game (Chess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ccd1b2-d8d1-4c31-bb29-fbc90c802eec",
   "metadata": {},
   "source": [
    "Anggota:\n",
    "- Muhammad Wisnu Maulana_162112233074\n",
    "- Anis Widya Astuti_162112233057\n",
    "- Nafisatul Nur Ismawati_162112233059\n",
    "- Rikza Maulana_162112233056 \n",
    "- Randy Afif_162112233081 \n",
    "\n",
    "\n",
    "Disini kami menggunakan pettingzoo, library pembelajaran multi agen. Kami memilih pettingzoo karena pettingzoo mirip dengan gymnasium, namun dengan support multi agen.\n",
    "\n",
    "Untuk framework deeplearningnya sendiri, kami mencoba menggunakan pytorch dan tensorflow. Namun, di akhir nanti akan kami fokuskan di salah satu yang terbaik saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06694b3b-e738-4cec-b439-e64feb1f587f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.10.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 00:50:13.902012: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-29 00:50:14.768221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-29 00:50:16.265579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tensordict import TensorDictBase\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import multiprocessing\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
    "from torchrl.modules import AdditiveGaussianWrapper, MultiAgentMLP, ProbabilisticActor, TanhDelta\n",
    "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
    "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
    "from torchrl.envs import check_env_specs, ExplorationType, PettingZooEnv, RewardSum, set_exploration_type, TransformedEnv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pygame\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "from pettingzoo.classic import chess_v6\n",
    "from pettingzoo.classic.chess.chess_utils import chess, get_move_plane\n",
    "\n",
    "# env = chess_v6.env(render_mode=\"human\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "try:\n",
    "    is_sphinx = __sphinx_build__\n",
    "except NameError:\n",
    "    is_sphinx = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41119963-127b-4d70-abf0-38b816224fbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Menggunakan Pytorch (masih error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f9b37e3-e534-4948-9adc-4530c03a7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "frames_per_batch = 1000\n",
    "n_iters = 5\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "iteration_when_stop_training_evaders = n_iters//2\n",
    "\n",
    "memory_size = 1000000\n",
    "n_optimiser_steps = 100\n",
    "train_batch_size = 128\n",
    "lr = 1e-3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "gamma = 0.99\n",
    "polyak_tau = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbcc9623-768c-4890-b57c-2d902942ffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: CompositeSpec(\n",
      "    player_0: CompositeSpec(\n",
      "        action: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=DiscreteBox(n=4672),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([1])),\n",
      "    player_1: CompositeSpec(\n",
      "        action: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=DiscreteBox(n=4672),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([1])), device=cpu, shape=torch.Size([]))\n",
      "reward_spec: CompositeSpec(\n",
      "    player_0: CompositeSpec(\n",
      "        reward: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([1])),\n",
      "    player_1: CompositeSpec(\n",
      "        reward: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous), device=cpu, shape=torch.Size([1])), device=cpu, shape=torch.Size([]))\n",
      "done_spec: CompositeSpec(\n",
      "    done: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    truncated: DiscreteTensorSpec(\n",
      "        shape=torch.Size([1]),\n",
      "        space=DiscreteBox(n=2),\n",
      "        device=cpu,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    player_0: CompositeSpec(\n",
      "        done: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        terminated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        truncated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([1])),\n",
      "    player_1: CompositeSpec(\n",
      "        done: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        terminated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        truncated: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([1])), device=cpu, shape=torch.Size([]))\n",
      "observation_spec: CompositeSpec(\n",
      "    player_0: CompositeSpec(\n",
      "        observation: CompositeSpec(\n",
      "            observation: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([1, 8, 8, 111]),\n",
      "                space=None,\n",
      "                device=cpu,\n",
      "                dtype=torch.bool,\n",
      "                domain=discrete), device=cpu, shape=torch.Size([1])),\n",
      "        action_mask: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 4672]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        mask: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([1])),\n",
      "    player_1: CompositeSpec(\n",
      "        observation: CompositeSpec(\n",
      "            observation: UnboundedContinuousTensorSpec(\n",
      "                shape=torch.Size([1, 8, 8, 111]),\n",
      "                space=None,\n",
      "                device=cpu,\n",
      "                dtype=torch.bool,\n",
      "                domain=discrete), device=cpu, shape=torch.Size([1])),\n",
      "        action_mask: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1, 4672]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete),\n",
      "        mask: DiscreteTensorSpec(\n",
      "            shape=torch.Size([1]),\n",
      "            space=DiscreteBox(n=2),\n",
      "            device=cpu,\n",
      "            dtype=torch.bool,\n",
      "            domain=discrete), device=cpu, shape=torch.Size([1])), device=cpu, shape=torch.Size([]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmaulana/.pyenv/versions/boardgame/lib/python3.10/site-packages/torchrl/envs/common.py:2989: DeprecationWarning: Your wrapper was not given a device. Currently, this value will default to 'cpu'. From v0.5 it will default to `None`. With a device of None, no device casting is performed and the resulting tensordicts are deviceless. Please set your device accordingly.\n",
      "  warnings.warn(\n",
      "/home/xmaulana/.pyenv/versions/boardgame/lib/python3.10/site-packages/torchrl/envs/libs/pettingzoo.py:1005: UserWarning: PettingZoo failed to load all modules with error message No module named 'multi_agent_ale_py', trying to load individual modules.\n",
      "  warnings.warn(\n",
      "/home/xmaulana/.pyenv/versions/boardgame/lib/python3.10/site-packages/torchrl/envs/libs/pettingzoo.py:56: UserWarning: SISL environments failed to load with error message No module named 'pymunk'.\n",
      "  warnings.warn(f\"SISL environments failed to load with error message {err}.\")\n",
      "/home/xmaulana/.pyenv/versions/boardgame/lib/python3.10/site-packages/torchrl/envs/libs/pettingzoo.py:68: UserWarning: Atari environments failed to load with error message No module named 'multi_agent_ale_py'.\n",
      "  warnings.warn(f\"Atari environments failed to load with error message {err}.\")\n",
      "/home/xmaulana/.pyenv/versions/boardgame/lib/python3.10/site-packages/torchrl/envs/libs/pettingzoo.py:74: UserWarning: Butterfly environments failed to load with error message No module named 'pymunk'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_steps = 100\n",
    "\n",
    "n_chasers = 2\n",
    "n_evaders = 1\n",
    "n_obstacles = 2\n",
    "\n",
    "base_env = PettingZooEnv(\n",
    "    task = \"chess_v6\",\n",
    "    parallel = False,\n",
    "    seed = seed,\n",
    "    use_mask = True\n",
    ")\n",
    "\n",
    "print(\"action_spec:\", base_env.full_action_spec)\n",
    "print(\"reward_spec:\", base_env.full_reward_spec)\n",
    "print(\"done_spec:\", base_env.full_done_spec)\n",
    "print(\"observation_spec:\", base_env.observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e052191-9f8b-43c2-818b-f3f20455d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('player_0', 'action'), ('player_1', 'action')]\n",
      "reward_keys: [('player_0', 'reward'), ('player_1', 'reward')]\n",
      "done_keys: ['done', 'terminated', 'truncated', ('player_0', 'done'), ('player_0', 'terminated'), ('player_0', 'truncated'), ('player_1', 'done'), ('player_1', 'terminated'), ('player_1', 'truncated')]\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", base_env.action_keys)\n",
    "print(\"reward_keys:\", base_env.reward_keys)\n",
    "print(\"done_keys:\", base_env.done_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6975fd7-b0a2-471b-9823-a8726f582afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    RewardSum(\n",
    "        in_keys=base_env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(base_env.group_map.keys()),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87ccc55a-993b-449d-9fe5-8709491a3482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-24 03:45:28,014 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed0fbcfc-f402-4356-9308-32e04b982f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "rollout of 5 steps: TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                player_0: TensorDict(\n",
      "                    fields={\n",
      "                        action_mask: Tensor(shape=torch.Size([1, 1, 4672]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        episode_reward: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        mask: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        observation: TensorDict(\n",
      "                            fields={\n",
      "                                observation: Tensor(shape=torch.Size([1, 1, 8, 8, 111]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                            batch_size=torch.Size([1, 1]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([1, 1]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                player_1: TensorDict(\n",
      "                    fields={\n",
      "                        action_mask: Tensor(shape=torch.Size([1, 1, 4672]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        done: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        episode_reward: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        mask: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        observation: TensorDict(\n",
      "                            fields={\n",
      "                                observation: Tensor(shape=torch.Size([1, 1, 8, 8, 111]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                            batch_size=torch.Size([1, 1]),\n",
      "                            device=cpu,\n",
      "                            is_shared=False),\n",
      "                        reward: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        terminated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                        truncated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([1, 1]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        player_0: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                action_mask: Tensor(shape=torch.Size([1, 1, 4672]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                mask: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: TensorDict(\n",
      "                    fields={\n",
      "                        observation: Tensor(shape=torch.Size([1, 1, 8, 8, 111]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([1, 1]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1, 1]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        player_1: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                action_mask: Tensor(shape=torch.Size([1, 1, 4672]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                done: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                episode_reward: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                mask: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                observation: TensorDict(\n",
      "                    fields={\n",
      "                        observation: Tensor(shape=torch.Size([1, 1, 8, 8, 111]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "                    batch_size=torch.Size([1, 1]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                truncated: Tensor(shape=torch.Size([1, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1, 1]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        truncated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=cpu,\n",
      "    is_shared=False)\n",
      "Shape of the rollout TensorDict: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(f\"rollout of {n_rollout_steps} steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07b260a7-056d-485b-80ab-359ef5f5bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = True  # Can change this based on the group\n",
    "\n",
    "    policy_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[\n",
    "            -1\n",
    "        ],\n",
    "        n_agent_outputs=env.full_action_spec[group, \"action\"].shape[\n",
    "            -1\n",
    "        ],\n",
    "        n_agents=len(agents),\n",
    "        centralised=False, \n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(group, \"observation\")],\n",
    "        out_keys=[(group, \"param\")],\n",
    "    ) \n",
    "    policy_modules[group] = policy_module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d15b2-0fab-4c32-a99d-964f8e201b7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Menggunakan Tensorflow (Masih saya uji coba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f931bbd-6a41-4f89-b258-0389230e6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 1e-3\n",
    "batch_size = 8\n",
    "memory_size = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2290cca-00af-42a3-ad68-8d706915bc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural net\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        # self.input_layer = tf.keras.Input(shape=(action_size,))\n",
    "        self.dense1 = tf.keras.layers.Dense(64, input_shape=(action_size,), activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(action_size, activation='softmax')\n",
    "        \n",
    "    def call(self, state):\n",
    "        # x = self.input_layer(state)\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.out(x)\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque(maxlen=size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.action_size = action_size\n",
    "        self.memory = ReplayMemory(memory_size)\n",
    "        self.model = QNetwork(action_size)\n",
    "        self.target_model = QNetwork(action_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            available = [i for i,x in enumerate(state) if x[0]]\n",
    "            # print(np.array(state[0][0][\"action_mask\"]))\n",
    "            randome = np.random.choice(available)\n",
    "            return randome\n",
    "        q_values = self.model(state)\n",
    "        print(q_values)\n",
    "        return np.argmax(q_values[0])\n",
    "    \n",
    "    def remember(self, state, action, reward, truncation):\n",
    "        self.memory.add((state, action, reward, truncation))\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = self.memory.sample(batch_size)\n",
    "        \n",
    "        for state, action, reward, truncation in minibatch:\n",
    "            target = self.model(state).numpy()\n",
    "            if truncation:\n",
    "                target[0][action] = reward\n",
    "            else:\n",
    "                t = self.target_model(state).numpy()\n",
    "                target[0][action] = reward + gamma * np.amax(t[0])\n",
    "                \n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = self.model(state)\n",
    "                loss = tf.keras.losses.MSE(target, q_values)\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        global epsilon\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded18118-543f-4710-9288-ad3cd72d1725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODE : 0\n",
      "EPISODE : 1\n"
     ]
    }
   ],
   "source": [
    "env = None\n",
    "is_raw = True\n",
    "\n",
    "if not is_raw:\n",
    "    env = chess_v6.env(render_mode=\"human\")\n",
    "else:\n",
    "    env = chess_v6.raw_env()\n",
    "env.reset(seed=42)\n",
    "episodes = 100\n",
    "running = True\n",
    "agents = {agent: DQNAgent(env.action_space(agent).n) for agent in env.agents}\n",
    "\n",
    "for i in range(episodes):\n",
    "    print(f\"EPISODE : {i}\")\n",
    "    env.reset(seed=42)\n",
    "    \n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if not is_raw:\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN:\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        termination = True\n",
    "                        running = False\n",
    "                        break\n",
    "                    \n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "            break\n",
    "        else:\n",
    "            mask = observation[\"action_mask\"]\n",
    "            mask = np.expand_dims(mask, axis=1)\n",
    "            \n",
    "            # available = [i for i,x in enumerate(mask) if x]\n",
    "            # print(available)\n",
    "            # action = np.random.choice(available)\n",
    "            \n",
    "            action = agents[agent].act(mask)\n",
    "            agents[agent].remember(mask, action, reward, truncation)\n",
    "            # if episodes > 0:\n",
    "            #     print(action)\n",
    "        \n",
    "        env.step(action)\n",
    "    for agent in agents:\n",
    "        agents[agent].replay(batch_size)\n",
    "    \n",
    "    if not running:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c28201-1515-4c9a-b0b8-d48e5dd8cb35",
   "metadata": {},
   "source": [
    "### Test dengan random move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84df37db-7bd4-4df5-8c49-56af0d8dda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = chess_v6.env(render_mode=\"human\")\n",
    "\n",
    "episodes = 100\n",
    "running = True\n",
    "for i in range(episodes):\n",
    "    print(f\"EPISODE : {i}\")\n",
    "    env.reset(seed=42)\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_ESCAPE:\n",
    "                    termination = True\n",
    "                    running = False\n",
    "                    break\n",
    "                    \n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "            break\n",
    "        else:\n",
    "            mask = observation[\"action_mask\"]\n",
    "            \n",
    "            available = [i for i,x in enumerate(mask) if x]\n",
    "            print(available)\n",
    "            action = np.random.choice(available)\n",
    "            # action = int(input(\"Move: \"))\n",
    "            # print(action)\n",
    "    \n",
    "        env.step(action)\n",
    "    if not running:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d419e0f2-de44-4757-bf41-db0a12fbb31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move = chess.Move(from_square=8*2+4, to_square=8*2+5, promotion=None)\n",
    "get_move_plane(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f32bb4a-1250-4123-92e7-eab7e52ad6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_move_plane(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453afe42-eb94-4269-8047-cb2991418f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-chess\n",
      "  Downloading python_chess-1.999-py3-none-any.whl.metadata (776 bytes)\n",
      "Requirement already satisfied: chess<2,>=1 in /home/xmaulana/.pyenv/versions/3.10.12/envs/boardgame/lib/python3.10/site-packages (from python-chess) (1.9.4)\n",
      "Downloading python_chess-1.999-py3-none-any.whl (1.4 kB)\n",
      "Installing collected packages: python-chess\n",
      "Successfully installed python-chess-1.999\n"
     ]
    }
   ],
   "source": [
    "!pip install python-chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91df276e-401c-4f3f-a13e-15c247afdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbb56ba-6d8f-4061-b1be-c1e619d4d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "board  = chess.Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d61b1aab-1b0f-48e5-8204-0f7a92eec66c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Board' object has no attribute 'can_claim_threeflod_repetition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcan_claim_threeflod_repetition\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Board' object has no attribute 'can_claim_threeflod_repetition'"
     ]
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23455a86-4c9e-4136-b834-1405c78691e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Move.from_uci('g8f6')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.push_san(\"Nf6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d1ae52-86fa-487f-9f7c-c37395290da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LegalMoveGenerator at 0x7f57a577fcd0 (Nh6, Nf6, Nc6, Na6, h6, g6, f6, e6, d6, c6, b6, a6, h5, g5, f5, e5, d5, c5, b5, a5)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.legal_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78cdeed3-ca71-4a97-9ac7-959b7b1b8af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.can_claim_threefold_repetition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34863b49-7054-4a78-9660-9e1298abde0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
